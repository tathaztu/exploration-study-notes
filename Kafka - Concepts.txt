ğ–ğ¡ğšğ­ ğ¢ğ¬ ğŠğšğŸğ¤ğš?

A very powerful and widely used event streaming platform

	Distributed Event Streaming Platform
	â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
	â€¢	Handle real-time data feeds, continuous flow of data
	â€¢ 	Offers publish, subscribe, store and process stream of records
	â€¢ 	High scalability, fault tolerance 
	
	Key Capabilities
	â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
	â€¢	Pub-Sub
	â€¢	Storage
		- Stores streams of records
		- Preserves order of events
	â€¢	Real time stream processing
		â†’ Intercommunication of data between systems
		â†’ Can handle massive amounts of data
	â€¢	Event-Driven Architectures
		â†’ Event Driven Microservice Architectures
	â€¢	Use Cases:
		â—‹ Real-time analytics
		â—‹ Fraud detection
		â—‹ Log aggregation
		â—‹ Stream processing
		â—‹ Website activity tracking
		â—‹ Financial transaction processing
		â—‹ IoT data collection.

ğŠğğ² ğ‚ğ¨ğ§ğœğğ©ğ­ğ¬

	â€¢	Topics: Categories, feeds to which records are published
	â€¢	Partition: 
		â—‹	Break-down of a Topic to managable size pieces
		â—‹ 	Scalability | Parallelism
		â—‹	Continually growsing, ordered, immutable sequence of records
	â€¢	Brokers:
		-	A Kafka cluster requires 1 or more broker(s)
		â—‹	Servers that comprise the Kafka cluster
		â—‹	Store the data

	â€¢	Producers
		â–ª Applications that publish (write) data to Kafka topics
		â–ª Choosing the Topic and corresponding Partition to send records to
	â€¢	Consumers:
		â–ª Applications that subscribe to (read) data from Kafka topics. 
		â–ª Track their position in each partition by maintaining an offset.

	â€¢	Consumer Groups:
		- Consumers can be organized into consumer groups 
		- Allows parallel consumption of data
		- Load Balancing | One consumer within a group per partition

	â€¢	Kafka Connect:
		- Framework 
		- Connects Kafka with external systems, - DB, data-sources, sinks, etc. 
		- Simplifies the process of importing and exporting data.
	
	â€¢	Kafka Streams:
		- Library for building stream processing applications. 
		- Process and transform data streams in real-time.
	
	â€¢	KRaft (Kafka Raft metadata mode):
		- New consensus mechanism that removes the Zookeeper dependancy. It is now recomended to deploy Kafka clusters using KRaft mode.


ğ‹ğğšğğğ« ğšğ§ğ ğˆğ’ğ‘

	ğ•ƒğ•–ğ•’ğ••ğ•–ğ•£
	â€¾â€¾â€¾â€¾â€¾â€¾â€¾
	ğğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğ‹ğğšğğğ«ğ¬ğ¡ğ¢ğ©:
		â—‹ Each partition within a Kafka topic has one designated leader 
		replica.
		â—‹ All read and write requests for that partition are handled by 
		this leader.
		â—‹ The leader is responsible for maintaining the authoritative 
		version of the partition's log.
	
	ğ‘ğ¨ğ¥ğ:
		â—‹ The leader receives data from producers and distributes it to the 
		follower replicas.
		â—‹ It also serves read requests from consumers.
		â—‹ This centralized handling simplifies data consistency.
	
	ğ•€ğ•Šâ„ (ğ•€ğ•Ÿ-ğ•Šğ•ªğ•Ÿğ•” â„ğ•–ğ•¡ğ•ğ•šğ•”ğ•’ğ•¤)
	â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
	ğ‘ğğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ’ğ²ğ§ğœğ¡ğ«ğ¨ğ§ğ¢ğ³ğšğ­ğ¢ğ¨ğ§:
		â—‹ Kafka replicates partitions across multiple brokers to ensure 
		data redundancy.
		â—‹ The ISR is a set of replicas that are "in sync" with the leader. 
		This means they have kept up with the leader's log.
	ğˆğ¦ğ©ğ¨ğ«ğ­ğšğ§ğœğ:
		â—‹ The ISR is crucial for data durability. If the leader fails, a 
		new leader is elected from the ISR.
		â—‹ Only replicas in the ISR are eligible for leader election, 
		minimizing the risk of data loss.
		â—‹ Kafka tracks which replicas are in the ISR and removes any that 
		fall behind (e.g., due to slow network or broker issues).
	ğŒğšğ¢ğ§ğ­ğšğ¢ğ§ğ¢ğ§ğ  ğˆğ’ğ‘:
		â—‹ Kafka maintains the ISR through information gathered from the 
		follower replicas. When a follower replica, keeps up to date with 
		the leader it remains in the ISR. If a follower falls behind, it 
		will be removed from the ISR.
	
	Summary:

	The leader is the primary replica that handles all reads and writes for 
	a partition.
	The ISR is the set of replicas that are currently synchronized with the 
	leader, ensuring data consistency and enabling fault tolerance.
	These components are essential for Kafka's ability to provide reliable 
	and highly available data streaming.

ğ‘ğ¨ğ¥ğ ğ¨ğŸ ğ™ğ¨ğ¨ğŠğğğ©ğğ«

Prior to Kafka version 2.8, and fully removed in version 4.0, Apache
Kafka heavily relied on Apache ZooKeeper for several critical
coordination and management tasks within the Kafka cluster. Think of
ZooKeeper as the central nervous system for the Kafka brokers. Here's a
breakdown of its purposes:

	ğŸ­. ğ—•ğ—¿ğ—¼ğ—¸ğ—²ğ—¿ ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜:

		Broker Registration: 
		When a Kafka broker starts up, it registers itself with ZooKeeper. This 
		allows other brokers and clients to discover the available brokers in 
		the cluster.
		
		Cluster Membership: 
		ZooKeeper maintains a dynamic list of all active brokers in the Kafka 
		cluster. If a broker goes down or a new one joins, ZooKeeper notifies 
		the other brokers about this change in topology.
	
	ğŸ®. ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹ğ—¹ğ—²ğ—¿ ğ—˜ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—»:

		Leader Election for the Controller: 
		One of the Kafka brokers is elected as the "controller." This controller 
		is responsible for managing partition leadership changes, topic 
		creation/deletion, and reassigning partitions. ZooKeeper plays a crucial 
		role in electing this controller. If the current controller fails, 
		ZooKeeper facilitates a new election among the remaining brokers.
	
	
	ğŸ¯. ğ—§ğ—¼ğ—½ğ—¶ğ—° ğ—®ğ—»ğ—± ğ—£ğ—®ğ—¿ğ˜ğ—¶ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜:

		Topic Configuration: 
		ZooKeeper stores metadata about Kafka topics, including the number of 
		partitions, replication factor, and any configuration overrides.
	
		Partition State: 
		ZooKeeper tracks the state of each partition, including which broker is 
		the current leader and which brokers are the in-sync replicas (ISRs).
	
	ğŸ°. ğ—–ğ—¼ğ—»ğ˜€ğ˜‚ğ—ºğ—²ğ—¿ ğ—šğ—¿ğ—¼ğ˜‚ğ—½ ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜ (ğ—³ğ—¼ğ—¿ ğ—¼ğ—¹ğ—±ğ—²ğ—¿ ğ—°ğ—¼ğ—»ğ˜€ğ˜‚ğ—ºğ—²ğ—¿ ğ—”ğ—£ğ—œğ˜€):

		Consumer Registration: 
		Older Kafka consumer APIs (before version 0.10) would register 
		themselves with ZooKeeper.
	
		Offset Storage: 
		ZooKeeper was also used to store the consumption offsets 
		for each consumer group, indicating how far each consumer had read in 
		each partition. Note: Modern Kafka consumers (0.10+) store their offsets 
		within Kafka itself in a dedicated internal topic (__consumer_offsets), 
		reducing the dependency on ZooKeeper for this specific task.
	
	ğŸ±. ğ—–ğ—¼ğ—»ğ—³ğ—¶ğ—´ğ˜‚ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—»ğ—± ğ—–ğ—¼ğ—¼ğ—¿ğ—±ğ—¶ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—»:

		Centralized Configuration: 
		ZooKeeper acts as a centralized repository for cluster-wide configurations.
	
		Notifications: 
		ZooKeeper provides a watch mechanism that allows Kafka brokers to be 
		notified of any changes in the metadata it manages. This enables timely 
		reactions to events like broker failures or topic modifications.
	
		Access Control Lists (ACLs): 
		ZooKeeper can store ACLs that define which users or applications have 
		permission to read or write to specific topics.
	
	In essence, ZooKeeper provided the necessary distributed
	coordination and consensus services that Kafka needed to manage its
	cluster state, elect leaders, and keep all the brokers in sync.

	ğ•‹ğ•™ğ•– ğ•Šğ•™ğ•šğ•—ğ•¥ ğ”¸ğ•¨ğ•’ğ•ª ğ•—ğ•£ğ• ğ• â„¤ğ• ğ• ğ•‚ğ•–ğ•–ğ•¡ğ•–ğ•£ (ğ•‚â„ğ•’ğ•—ğ•¥ ğ•„ğ• ğ••ğ•–)
	â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”

	It's important to note that Kafka is actively moving away from its
	dependency on ZooKeeper with the introduction of KRaft (Kafka Raft
	metadata mode). This new consensus mechanism integrates metadata
	management directly into the Kafka brokers themselves, eliminating the
	need for a separate ZooKeeper cluster.

	As of Kafka version 3.5, ZooKeeper is deprecated, and Kafka 4.0 will
	not support ZooKeeper at all. The KRaft mode simplifies the
	architecture, reduces operational complexity, and improves performance
	and security.

	Therefore, while ZooKeeper was historically crucial for Kafka, its role
	is being phased out in favor of a self-managed metadata quorum within
	the Kafka cluster.


ğ‚ğšğ§ğ§ğ¨ğ­ ğ¦ğ¨ğğ¢ğŸğ² ğ‘ğğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§-ğ…ğšğœğ­ğ¨ğ«

	Once a topic is created in Apache Kafka, you cannot directly change its 
	replication factor. This is a significant design decision with implications for data durability and consistency. The following are the reasons behind this limitation along with the workarounds:


	ğ—–ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ˜ğ˜† ğ—¼ğ—³ ğ——ğ—®ğ˜ğ—® ğ—¥ğ—²ğ—±ğ—¶ğ˜€ğ˜ğ—¿ğ—¶ğ—¯ğ˜‚ğ˜ğ—¶ğ—¼ğ—»
	Changing the replication factor after topic creation would necessitate a 
	potentially massive data redistribution across the Kafka brokers. This 
	process would be:
		ğ‘»ğ’Šğ’ğ’†-ğ’„ğ’ğ’ğ’”ğ’–ğ’ğ’Šğ’ğ’ˆ: Especially for large topics with significant data 
		volumes.
		ğ‘¹ğ’†ğ’”ğ’ğ’–ğ’“ğ’„ğ’†-ğ’Šğ’ğ’•ğ’†ğ’ğ’”ğ’Šğ’—ğ’†: It would put a heavy load on the network, disk I/O, 
		and CPU of the brokers.
		ğ‘ªğ’ğ’ğ’‘ğ’ğ’†ğ’™ ğ’•ğ’ ğ’ğ’‚ğ’ğ’‚ğ’ˆğ’†: Ensuring data consistency and preventing data loss 
		during this redistribution would be a non-trivial engineering challenge. 
		Handling failures during the process would add further complexity.

	ğ—œğ—ºğ—½ğ—®ğ—°ğ˜ ğ—¼ğ—» ğ—£ğ—®ğ—¿ğ˜ğ—¶ğ˜ğ—¶ğ—¼ğ—» ğ—Ÿğ—²ğ—®ğ—±ğ—²ğ—¿ğ˜€ğ—µğ—¶ğ—½ ğ—®ğ—»ğ—± ğ—œğ—¦ğ—¥: 
	Altering the replication factor would directly affect the number of replicas 
	for each partition. This would require recalculating and potentially 
	reassigning partition leaders and the In-Sync Replica (ISR) set for all 
	affected partitions. 
	This process needs careful coordination to avoid disruptions.

	ğ—£ğ—¼ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ ğ—³ğ—¼ğ—¿ ğ—œğ—»ğ—°ğ—¼ğ—»ğ˜€ğ—¶ğ˜€ğ˜ğ—²ğ—»ğ—°ğ—¶ğ—²ğ˜€
	Allowing dynamic changes to the replication factor could introduce 
	complexities 
	in maintaining strong consistency guarantees, especially during the 
	transition period.

	ğ—¦ğ—¶ğ—ºğ—½ğ—¹ğ—¶ğ—°ğ—¶ğ˜ğ˜† ğ—¼ğ—³ ğ—œğ—»ğ—¶ğ˜ğ—¶ğ—®ğ—¹ ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—»:
	The initial design of Kafka prioritized simplicity and performance for its 
	core publish-subscribe functionality. Allowing dynamic replication factor 
	changes would have added significant complexity to the core architecture.

ğ–ğ¨ğ«ğ¤ğšğ«ğ¨ğ®ğ§ğğ¬ ğŸğ¨ğ« ğ‚ğ¡ğšğ§ğ ğ¢ğ§ğ  ğ­ğ¡ğ ğ‘ğğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ ğ…ğšğœğ­ğ¨ğ«

	Since you cannot directly modify the replication factor of an existing topic, you need to employ workarounds to achieve the desired outcome:

	ğ—–ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—® ğ—¡ğ—²ğ˜„ ğ—§ğ—¼ğ—½ğ—¶ğ—° ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ——ğ—²ğ˜€ğ—¶ğ—¿ğ—²ğ—± ğ—¥ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—™ğ—®ğ—°ğ˜ğ—¼ğ—¿:

	â€¢	This is the most common and recommended approach.
	â€¢ 	Create a new topic with the same configuration as the original topic 	
		(number of partitions, etc.) but with the desired replication factor.
	â€¢	Migrate the data from the old topic to the new topic. This can be done 	
		using various methods:
		â–«	ğ‘²ğ’‚ğ’‡ğ’Œğ’‚ ğ‘´ğ’Šğ’“ğ’“ğ’ğ’“ğ‘´ğ’‚ğ’Œğ’†ğ’“ (ğ’—ğ’†ğ’“ğ’”ğ’Šğ’ğ’ 1): A tool specifically designed for mirroring topics between Kafka clusters. It can also be used to copy data within the same cluster.
		â–«	ğ‘²ğ’‚ğ’‡ğ’Œğ’‚ ğ‘´ğ’Šğ’“ğ’“ğ’ğ’“ğ‘´ğ’‚ğ’Œğ’†ğ’“ 2 (ğ‘ªğ’ğ’ğ’ğ’†ğ’„ğ’•-ğ’ƒğ’‚ğ’”ğ’†ğ’…): The newer version of MirrorMaker built on Kafka Connect, offering more flexibility and features.
		â–« 	ğ‘ªğ’–ğ’”ğ’•ğ’ğ’ ğ‘ªğ’ğ’ğ’”ğ’–ğ’ğ’†ğ’“/ğ‘·ğ’“ğ’ğ’…ğ’–ğ’„ğ’†ğ’“ ğ‘¨ğ’‘ğ’‘ğ’ğ’Šğ’„ğ’‚ğ’•ğ’Šğ’ğ’:: You can write a custom application that consumes data from the old topic and produces it to the new topic.
	â€¢ 	Once the migration is complete and verified, you can switch your consumers and producers to use the new topic and eventually delete the old one.

	ğ— ğ—®ğ—»ğ˜‚ğ—®ğ—¹ ğ—£ğ—®ğ—¿ğ˜ğ—¶ğ˜ğ—¶ğ—¼ğ—» ğ—¥ğ—²ğ—®ğ˜€ğ˜€ğ—¶ğ—´ğ—»ğ—ºğ—²ğ—»ğ˜ (ğ—”ğ—±ğ˜ƒğ—®ğ—»ğ—°ğ—²ğ—± ğ—®ğ—»ğ—± ğ—¥ğ—¶ğ˜€ğ—¸ğ˜†)

	â€¢	Kafka provides tools for manually reassigning partitions between brokers. While this can indirectly help in achieving a higher replication factor, it's a complex and error-prone process that requires careful planning and execution.
	â€¢	You would need to manually move partitions to brokers that don't currently host replicas of those partitions.
	â€¢	This method is generally not recommended for simply changing the replication factor and is more suited for rebalancing partitions across the cluster.

	ğ—œğ—ºğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—Ÿğ—¶ğ—ºğ—¶ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»:

	â€¢	ğ‘·ğ’ğ’‚ğ’ğ’ğ’Šğ’ğ’ˆ ğ’Šğ’” ğ‘ªğ’“ğ’–ğ’„ğ’Šğ’‚ğ’: You need to carefully consider the required level of data durability and fault tolerance when creating a topic and set the replication factor accordingly.
	â€¢	ğ‘«ğ’‚ğ’•ğ’‚ ğ‘´ğ’Šğ’ˆğ’“ğ’‚ğ’•ğ’Šğ’ğ’ ğ‘¶ğ’—ğ’†ğ’“ğ’‰ğ’†ğ’‚ğ’…: Changing the effective replication factor involves data migration, which can be time-consuming and resource-intensive.
	â€¢	ğ‘¶ğ’‘ğ’†ğ’“ğ’‚ğ’•ğ’Šğ’ğ’ğ’‚ğ’ ğ‘ªğ’ğ’ğ’‘ğ’ğ’†ğ’™ğ’Šğ’•ğ’š: Managing data migration adds to the operational overhead of maintaining the Kafka cluster.


In summary, the inability to directly change the replication factor of a Kafka topic after creation is a deliberate design choice rooted in the complexity of data redistribution, potential inconsistencies, and the initial architectural focus on core streaming functionality. The recommended approach to achieve a different replication factor is to create a new topic with the desired configuration and migrate the data from the old topic.